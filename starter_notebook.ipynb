{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
        "\n",
        "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "72b89f0e-66be-406d-9200-43fa4d446262"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"urh\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef059359-bd07-4591-a60d-7df3df6237b9"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-urh-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8210f6a2-14bb-4be9-abd3-2c7abd844458"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.0MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "e9a0931f-3b3b-4953-db1b-c4394143c813"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-urh.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            " 304 KB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-urh.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "   3 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/urh.zip\n",
            "\n",
            " 267 MB Total size\n",
            "./JW300_latest_xml_en-urh.xml.gz ... 100% of 304 KB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_urh.zip ... 100% of 3 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n48GDRnP8y2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "b68cdd88-6c52-4f6a-ce6d-1c46ab2f4b30"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-17 10:39:35--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en’\n",
            "\n",
            "\rtest.en-any.en        0%[                    ]       0  --.-KB/s               \rtest.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-07-17 10:39:35 (5.08 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
            "\n",
            "--2020-07-17 10:39:36--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-urh.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 201504 (197K) [text/plain]\n",
            "Saving to: ‘test.en-urh.en’\n",
            "\n",
            "test.en-urh.en      100%[===================>] 196.78K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-07-17 10:39:37 (4.48 MB/s) - ‘test.en-urh.en’ saved [201504/201504]\n",
            "\n",
            "--2020-07-17 10:39:39--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-urh.urh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 236859 (231K) [text/plain]\n",
            "Saving to: ‘test.en-urh.urh’\n",
            "\n",
            "test.en-urh.urh     100%[===================>] 231.31K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-07-17 10:39:39 (4.51 MB/s) - ‘test.en-urh.urh’ saved [236859/236859]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NqDG-CI28y2L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63ae0818-0ac4-4906-c23e-e847e1dfe169"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "f4c26f94-42c9-4d6e-a67e-669ad3a66561"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 4050/32709 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Why It Pays to Be Honest 6</td>\n",
              "      <td>Erere Herọ Ra Vwọ Dia Ohwo rẹ Uyota 5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Bible Changes Lives</td>\n",
              "      <td>7 Ovwan “ Jẹn Ẹguọnọ rẹ Iniọvo na Dje Ebuoebuo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Give Me Just One Year of Peace and Happiness 8...</td>\n",
              "      <td>12 Jẹ ‘ Ẹse rẹ Ọghẹnẹ rẹ Unu se Gbe - e na , ’...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0                         Why It Pays to Be Honest 6              Erere Herọ Ra Vwọ Dia Ohwo rẹ Uyota 5\n",
              "1                            The Bible Changes Lives  7 Ovwan “ Jẹn Ẹguọnọ rẹ Iniọvo na Dje Ebuoebuo...\n",
              "2  Give Me Just One Year of Peace and Happiness 8...  12 Jẹ ‘ Ẹse rẹ Ọghẹnẹ rẹ Unu se Gbe - e na , ’..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjxHg-UZxZpA",
        "colab_type": "text"
      },
      "source": [
        "The first translation is accurate but the following two are incorrect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "13d32d21-8276-4c11-b220-76aefdb879e7"
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_1BwAApEtMk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "37a0259e-5ca5-4a47-d9aa-d5ed11f5ada8"
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "from os import cpu_count\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (49.1.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144803 sha256=d511be8c4feb575921062b235204ee953d4b8d34368315baf44277dab2abca47\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fHh9VQEn7MS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1acf96e7-134a-45d6-beaf-ac4a48b3b7e9"
      },
      "source": [
        "start_time = time.time()\n",
        "### iterating over pandas dataframe rows is not recomended, let use multi processing to apply the function\n",
        "\n",
        "with Pool(cpu_count()-1) as pool:\n",
        "    scores = pool.map(partial(fuzzfilter, candidates=list(en_test_sents), pad=5), df_pp['source_sentence'])\n",
        "hours, rem = divmod(time.time() - start_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"done in {}h:{}min:{}seconds\".format(hours, minutes, seconds))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp = df_pp.assign(scores=scores)\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '*']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done in 0.0h:10.0min:20.97909140586853seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "a94e37b5-6cfb-4eff-f66d-7e272c83d698"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "The number of publishers is now about ten times what it was when I began serving here .\n",
            "Similarly , elders should not only encourage and console their brothers with words but also build them up by showing sincere personal interest . ​ — 1 Cor .\n",
            "17 Why We “ Keep Bearing Much Fruit ”\n",
            "Now I have a journal that I keep on my desk to schedule upcoming work , and this helps me to schedule myself , not leaving things till the last minute . ”\n",
            "1 , 2 . ( a ) How do some react to the thought that God has an organization ?\n",
            "We cannot go to the point of disobeying God or violating our Christian neutrality . ​ — Read 1 Peter 2 : 13 - 17 .\n",
            "Did this mean freedom for every literal slave ?\n",
            "Eventually , all my siblings did so and became Jehovah’s Witnesses .\n",
            "How pleased Jehovah will be as he observes our whole - souled efforts to “ keep bearing much fruit ” !\n",
            "Joseph , though , was a disciple , but he could not bring himself to say so openly .\n",
            "\n",
            "==> train.urh <==\n",
            "Ighwoghwota rehẹ ẹkuotọ na enẹna vwẹ ọhwọhwọ ihwe vwo bun vrẹ obo rọ hepha ọke me vwọ ga vwẹ oboyin .\n",
            "( 2 Kọr . 12 : 15 ) Vwẹ idjerhe vuọvo na , vwọ vrẹ ota rẹ unu rẹ ekpako cha vwọ bọn iniọvo na gan , o ji fo nẹ ayen ru obo ro che djephia nẹ ayen vwo ọdavwẹ rayen . ​ — 1 Kọr .\n",
            "17 Oboresorọ O Vwo Fo Nẹ A “ Mọ Ibi Buebu ”\n",
            "Asaọkiephana , mi vwo ẹbe rẹ mi si ọrhuẹrẹphiyotọ mẹ phiyọ , ọnana vwẹ ukẹcha kẹ vwẹ vwọ nabọ vwẹrote iruo mẹ , me rha yanjẹ ọvuọvo vwo hẹrhẹ imibrẹro ri chekọ bẹsiẹ ọke na vwo re - e . ”\n",
            "1 , 2 . ( a ) Die yen ihwo evo ta siẹrẹ ayen de nyo nẹ Ọghẹnẹ vwo ukoko ?\n",
            "Avwanre cha sa churhi rẹ Ọghẹnẹ fikirẹ aye - en yẹrẹ dia ẹbẹre ọvo rẹ akpọ na - a . — Se 1 Pita 2 : 13 - 17 .\n",
            "( Luk 4 : 18 ) Ọnana mudiaphiyọ egbomọphẹ vwọ kẹ ihwo re mu kpo eviẹn ?\n",
            "Ukuotọ rọyen , iniọvo mẹ eje de yono Baibol ji bromaphiyame kerẹ Iseri rẹ Jihova .\n",
            "O muẹro dẹn nẹ oma nabọ vwerhen Jihova kọke kọke rọ da mrẹ oborẹ avwanre davwan te , ra vwọ “ mọ ibi buebu ” !\n",
            "Ẹkẹvuọvo , Josẹf ọyen odibo rẹ Jesu ro se dje oma phia vwẹ azagba - a .\n",
            "==> dev.en <==\n",
            "These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "Today he is serving at Bethel .\n",
            "But freedom from what ?\n",
            "Avoid comparing your new congregation with your previous one .\n",
            "2 : 16 , 17 .\n",
            "As stated , the vindication of Jehovah’s sovereignty is a vital issue involving mankind .\n",
            "That is especially so if our treacherous heart tugs us in the opposite direction .\n",
            "At times , this resulted in more money going out than coming in for a period of time .\n",
            "How did hope reinforce Noah’s faith ?\n",
            "What prompts a mother to care tenderly for her newborn baby ?\n",
            "\n",
            "==> dev.urh <==\n",
            "E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2 : 16 , 17 .\n",
            "Kirobo ra tarọ jovwo , etito rẹ usuon rẹ Jihova , ọyen ota ọghanghanre ro fori nẹ ihworakpọ tẹnrovi .\n",
            "Ma rho , udu avwanre rọ vọnre vẹ ophiẹnvwe na da vuẹ avwanre nẹ e ru obo re chọre .\n",
            "Iruo kpokpọ nana nẹrhẹ a ghwọrọ vrẹ obo re torori ọkiọvo .\n",
            "Mavọ yen iphiẹrophiyọ vwọ nẹrhẹ esegbuyota rẹ Noa ganphiyọ ?\n",
            "Die yen mu oni vwọ vwẹrote ọmọ ro ghwe vwiẹ ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d99cec69-6929-49ba-966b-7f5d39d45d10"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 2467, done.\u001b[K\n",
            "remote: Total 2467 (delta 0), reused 0 (delta 0), pack-reused 2467\u001b[K\n",
            "Receiving objects: 100% (2467/2467), 2.64 MiB | 22.49 MiB/s, done.\n",
            "Resolving deltas: 100% (1725/1725), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (7.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (49.1.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.5.1+cu101)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (2.2.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/5b/cf661da8e9b0229f5d98c2961b072a5728fd11a0758957f8c0fd36081c06/sacrebleu-1.4.12-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.10.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 6.8MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/fb/734960c55474c8f74e6ad4c8588fc44073fb9d69e223269d26a3c2435d16/pylint-2.5.3-py3-none-any.whl (324kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 24.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.30.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.12.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.41.1)\n",
            "Collecting mecab-python3==0.996.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 202kB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/e7/ceef002a300a98a208232fab593183249b6964b306ee7dabb29908419cca/portalocker-1.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.0.5)\n",
            "Collecting toml>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
            "Collecting astroid<=2.5,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/a8/5133f51967fb21e46ee50831c3f5dda49e976b7f915408d670b1603d41d6/astroid-2.4.2-py3-none-any.whl (213kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 43.3MB/s \n",
            "\u001b[?25hCollecting isort<5,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 37.3MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Building wheels for collected packages: joeynmt, pyyaml, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=77293 sha256=fca83472fba61692e12f83ed720302a478766edfcb14be9b53acc96a868b3f81\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nvdmaljd/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=ead0ea7c45a20f22eff4d052e58ef00cf9d62abcbdc022db4879ff4fadeac09a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp36-cp36m-linux_x86_64.whl size=67434 sha256=4f28f719f56894ab5e4c1646d88cc2c9ba51b557830aeed96808b7c958b01d91\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built joeynmt pyyaml wrapt\n",
            "Installing collected packages: mecab-python3, portalocker, sacrebleu, subword-nmt, pyyaml, toml, typed-ast, wrapt, lazy-object-proxy, astroid, isort, mccabe, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "Successfully installed astroid-2.4.2 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 mecab-python3-0.996.5 portalocker-1.7.1 pylint-2.5.3 pyyaml-5.3.1 sacrebleu-1.4.12 subword-nmt-0.3.7 toml-0.10.1 typed-ast-1.4.1 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "04b98d0d-7c39-42ac-f6b3-9cfa2315ed60"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Xhosa Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.urh    test.urh\t    train.en\n",
            "dev.bpe.en\tdev.urh      test.en\t     train.bpe.en   train.urh\n",
            "dev.bpe.urh\ttest.bpe.en  test.en-any.en  train.bpe.urh\n",
            "bpe.codes.4000\tdev.en\t test.bpe.en   test.en-any.en  train.bpe.urh\n",
            "dev.bpe.en\tdev.urh  test.bpe.urh  test.urh        train.en\n",
            "dev.bpe.urh\tmodels\t test.en       train.bpe.en    train.urh\n",
            "BPE Xhosa Sentences\n",
            "Diesorọ H@@ us@@ ha@@ i vwọ guọnọ uduefiogbere ọ sa vwọ fuevun kẹ Ọghẹnẹ ?\n",
            "Diesorọ ọ vwọ guọnọ uduefiogbere avwanre ke sa fuevun ?\n",
            "Me nẹrhovwo vwọ kẹ uduefiogbere me sa vwọ yọn@@ regan .\n",
            "E@@ nẹ@@ na , ẹwẹn rayen kpotọ re , me sa kọn bru ayen ra ọkieje . ” — Se Isẹ 29 : 25 .\n",
            "[ 1 ] ( ẹkorota 7 ) E wene e@@ dẹ evo .\n",
            "Combined BPE Vocab\n",
            "ording\n",
            "ople\n",
            "eignty\n",
            "ẹgbaẹ@@\n",
            "Heb@@\n",
            "sider\n",
            "Babil@@\n",
            "/@@\n",
            "rọvw@@\n",
            "Jihov@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c70b07f7-1350-49a4-bda7-a8c7cac83e7b"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t test.bpe.en   test.en-any.en  train.bpe.urh\n",
            "dev.bpe.en\tdev.urh  test.bpe.urh  test.urh        train.en\n",
            "dev.bpe.urh\tmodels\t test.en       train.bpe.en    train.urh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5ee42ce-f3a9-4c16-b001-a2a01ead2d5c"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-07-17 10:51:31,059 Hello! This is Joey-NMT.\n",
            "2020-07-17 10:51:31.183210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-17 10:51:32,686 Total params: 12119552\n",
            "2020-07-17 10:51:32,688 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2020-07-17 10:51:48,538 cfg.name                           : enurh_transformer\n",
            "2020-07-17 10:51:48,538 cfg.data.src                       : en\n",
            "2020-07-17 10:51:48,538 cfg.data.trg                       : urh\n",
            "2020-07-17 10:51:48,538 cfg.data.train                     : data/enurh/train.bpe\n",
            "2020-07-17 10:51:48,538 cfg.data.dev                       : data/enurh/dev.bpe\n",
            "2020-07-17 10:51:48,538 cfg.data.test                      : data/enurh/test.bpe\n",
            "2020-07-17 10:51:48,538 cfg.data.level                     : bpe\n",
            "2020-07-17 10:51:48,539 cfg.data.lowercase                 : False\n",
            "2020-07-17 10:51:48,539 cfg.data.max_sent_length           : 100\n",
            "2020-07-17 10:51:48,539 cfg.data.src_vocab                 : data/enurh/vocab.txt\n",
            "2020-07-17 10:51:48,539 cfg.data.trg_vocab                 : data/enurh/vocab.txt\n",
            "2020-07-17 10:51:48,539 cfg.testing.beam_size              : 5\n",
            "2020-07-17 10:51:48,539 cfg.testing.alpha                  : 1.0\n",
            "2020-07-17 10:51:48,539 cfg.training.random_seed           : 42\n",
            "2020-07-17 10:51:48,539 cfg.training.optimizer             : adam\n",
            "2020-07-17 10:51:48,540 cfg.training.normalization         : tokens\n",
            "2020-07-17 10:51:48,540 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2020-07-17 10:51:48,540 cfg.training.scheduling            : plateau\n",
            "2020-07-17 10:51:48,540 cfg.training.patience              : 5\n",
            "2020-07-17 10:51:48,540 cfg.training.learning_rate_factor  : 0.5\n",
            "2020-07-17 10:51:48,540 cfg.training.learning_rate_warmup  : 1000\n",
            "2020-07-17 10:51:48,540 cfg.training.decrease_factor       : 0.7\n",
            "2020-07-17 10:51:48,541 cfg.training.loss                  : crossentropy\n",
            "2020-07-17 10:51:48,541 cfg.training.learning_rate         : 0.0003\n",
            "2020-07-17 10:51:48,541 cfg.training.learning_rate_min     : 1e-08\n",
            "2020-07-17 10:51:48,541 cfg.training.weight_decay          : 0.0\n",
            "2020-07-17 10:51:48,541 cfg.training.label_smoothing       : 0.1\n",
            "2020-07-17 10:51:48,541 cfg.training.batch_size            : 4096\n",
            "2020-07-17 10:51:48,541 cfg.training.batch_type            : token\n",
            "2020-07-17 10:51:48,541 cfg.training.eval_batch_size       : 3600\n",
            "2020-07-17 10:51:48,541 cfg.training.eval_batch_type       : token\n",
            "2020-07-17 10:51:48,542 cfg.training.batch_multiplier      : 1\n",
            "2020-07-17 10:51:48,542 cfg.training.early_stopping_metric : ppl\n",
            "2020-07-17 10:51:48,542 cfg.training.epochs                : 30\n",
            "2020-07-17 10:51:48,542 cfg.training.validation_freq       : 1000\n",
            "2020-07-17 10:51:48,542 cfg.training.logging_freq          : 100\n",
            "2020-07-17 10:51:48,542 cfg.training.eval_metric           : bleu\n",
            "2020-07-17 10:51:48,542 cfg.training.model_dir             : models/enurh_transformer\n",
            "2020-07-17 10:51:48,542 cfg.training.overwrite             : False\n",
            "2020-07-17 10:51:48,542 cfg.training.shuffle               : True\n",
            "2020-07-17 10:51:48,543 cfg.training.use_cuda              : True\n",
            "2020-07-17 10:51:48,543 cfg.training.max_output_length     : 100\n",
            "2020-07-17 10:51:48,543 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2020-07-17 10:51:48,543 cfg.training.keep_last_ckpts       : 3\n",
            "2020-07-17 10:51:48,543 cfg.model.initializer              : xavier\n",
            "2020-07-17 10:51:48,543 cfg.model.bias_initializer         : zeros\n",
            "2020-07-17 10:51:48,543 cfg.model.init_gain                : 1.0\n",
            "2020-07-17 10:51:48,543 cfg.model.embed_initializer        : xavier\n",
            "2020-07-17 10:51:48,543 cfg.model.embed_init_gain          : 1.0\n",
            "2020-07-17 10:51:48,544 cfg.model.tied_embeddings          : True\n",
            "2020-07-17 10:51:48,544 cfg.model.tied_softmax             : True\n",
            "2020-07-17 10:51:48,544 cfg.model.encoder.type             : transformer\n",
            "2020-07-17 10:51:48,544 cfg.model.encoder.num_layers       : 6\n",
            "2020-07-17 10:51:48,544 cfg.model.encoder.num_heads        : 4\n",
            "2020-07-17 10:51:48,544 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2020-07-17 10:51:48,544 cfg.model.encoder.embeddings.scale : True\n",
            "2020-07-17 10:51:48,544 cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2020-07-17 10:51:48,544 cfg.model.encoder.hidden_size      : 256\n",
            "2020-07-17 10:51:48,545 cfg.model.encoder.ff_size          : 1024\n",
            "2020-07-17 10:51:48,545 cfg.model.encoder.dropout          : 0.3\n",
            "2020-07-17 10:51:48,545 cfg.model.decoder.type             : transformer\n",
            "2020-07-17 10:51:48,545 cfg.model.decoder.num_layers       : 6\n",
            "2020-07-17 10:51:48,545 cfg.model.decoder.num_heads        : 4\n",
            "2020-07-17 10:51:48,545 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2020-07-17 10:51:48,545 cfg.model.decoder.embeddings.scale : True\n",
            "2020-07-17 10:51:48,545 cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2020-07-17 10:51:48,546 cfg.model.decoder.hidden_size      : 256\n",
            "2020-07-17 10:51:48,546 cfg.model.decoder.ff_size          : 1024\n",
            "2020-07-17 10:51:48,546 cfg.model.decoder.dropout          : 0.3\n",
            "2020-07-17 10:51:48,546 Data set sizes: \n",
            "\ttrain 25605,\n",
            "\tvalid 1000,\n",
            "\ttest 2652\n",
            "2020-07-17 10:51:48,546 First training example:\n",
            "\t[SRC] The number of publishers is now about ten times what it was when I began serving here .\n",
            "\t[TRG] I@@ ghwoghwota rehẹ ẹkuotọ na enẹna vwẹ ọh@@ wọ@@ h@@ wọ ihwe vwo bun vrẹ obo rọ hepha ọke me vwọ ga vwẹ oboyin .\n",
            "2020-07-17 10:51:48,546 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) rẹ (7) the (8) to (9) na\n",
            "2020-07-17 10:51:48,547 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) rẹ (7) the (8) to (9) na\n",
            "2020-07-17 10:51:48,547 Number of Src words (types): 4138\n",
            "2020-07-17 10:51:48,548 Number of Trg words (types): 4138\n",
            "2020-07-17 10:51:48,548 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4138),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4138))\n",
            "2020-07-17 10:51:48,551 EPOCH 1\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n",
            "2020-07-17 10:52:01,826 Epoch   1 Step:      100 Batch Loss:     5.358685 Tokens per Sec:    15718, Lr: 0.000300\n",
            "2020-07-17 10:52:14,851 Epoch   1 Step:      200 Batch Loss:     4.839056 Tokens per Sec:    16023, Lr: 0.000300\n",
            "2020-07-17 10:52:27,223 Epoch   1: total training loss 1518.10\n",
            "2020-07-17 10:52:27,223 EPOCH 2\n",
            "2020-07-17 10:52:27,764 Epoch   2 Step:      300 Batch Loss:     4.534461 Tokens per Sec:    13881, Lr: 0.000300\n",
            "2020-07-17 10:52:40,583 Epoch   2 Step:      400 Batch Loss:     4.657644 Tokens per Sec:    16269, Lr: 0.000300\n",
            "2020-07-17 10:52:53,274 Epoch   2 Step:      500 Batch Loss:     4.424926 Tokens per Sec:    16127, Lr: 0.000300\n",
            "2020-07-17 10:53:05,150 Epoch   2: total training loss 1289.55\n",
            "2020-07-17 10:53:05,150 EPOCH 3\n",
            "2020-07-17 10:53:06,228 Epoch   3 Step:      600 Batch Loss:     4.099325 Tokens per Sec:    15826, Lr: 0.000300\n",
            "2020-07-17 10:53:19,196 Epoch   3 Step:      700 Batch Loss:     3.856677 Tokens per Sec:    16054, Lr: 0.000300\n",
            "2020-07-17 10:53:32,111 Epoch   3 Step:      800 Batch Loss:     3.903601 Tokens per Sec:    15871, Lr: 0.000300\n",
            "2020-07-17 10:53:43,723 Epoch   3: total training loss 1141.67\n",
            "2020-07-17 10:53:43,723 EPOCH 4\n",
            "2020-07-17 10:53:45,226 Epoch   4 Step:      900 Batch Loss:     3.671206 Tokens per Sec:    15896, Lr: 0.000300\n",
            "2020-07-17 10:53:58,220 Epoch   4 Step:     1000 Batch Loss:     3.670401 Tokens per Sec:    16212, Lr: 0.000300\n",
            "2020-07-17 10:54:25,177 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 10:54:25,178 Saving new checkpoint.\n",
            "2020-07-17 10:54:25,429 Example #0\n",
            "2020-07-17 10:54:25,429 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 10:54:25,429 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 10:54:25,430 \tHypothesis: ( 1 : 1 ) Ẹkẹvuọvo , avwanre de vwo ẹguọnọ rẹ avwanre vwo ẹguọnọ rẹ avwanre .\n",
            "2020-07-17 10:54:25,430 Example #1\n",
            "2020-07-17 10:54:25,430 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 10:54:25,430 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 10:54:25,430 \tHypothesis: ( 1 : 1 . 4 ) Ẹkẹvuọvo , ọ da dia ohwo rẹ avwanre .\n",
            "2020-07-17 10:54:25,430 Example #2\n",
            "2020-07-17 10:54:25,431 \tSource:     But freedom from what ?\n",
            "2020-07-17 10:54:25,431 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 10:54:25,431 \tHypothesis: Die yen avwanre vwo ru ?\n",
            "2020-07-17 10:54:25,431 Example #3\n",
            "2020-07-17 10:54:25,431 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 10:54:25,431 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 10:54:25,432 \tHypothesis: ( 1 : 1 ) Ẹkẹvuọvo , avwanre de vwo ẹguọnọ rẹ avwanre .\n",
            "2020-07-17 10:54:25,432 Validation result (greedy) at epoch   4, step     1000: bleu:   1.16, loss: 85283.7500, ppl:  37.1493, duration: 27.2115s\n",
            "2020-07-17 10:54:38,173 Epoch   4 Step:     1100 Batch Loss:     3.585031 Tokens per Sec:    15598, Lr: 0.000300\n",
            "2020-07-17 10:54:48,950 Epoch   4: total training loss 1068.96\n",
            "2020-07-17 10:54:48,951 EPOCH 5\n",
            "2020-07-17 10:54:50,902 Epoch   5 Step:     1200 Batch Loss:     3.495416 Tokens per Sec:    16686, Lr: 0.000300\n",
            "2020-07-17 10:55:03,808 Epoch   5 Step:     1300 Batch Loss:     3.260602 Tokens per Sec:    16053, Lr: 0.000300\n",
            "2020-07-17 10:55:16,538 Epoch   5 Step:     1400 Batch Loss:     3.380402 Tokens per Sec:    16147, Lr: 0.000300\n",
            "2020-07-17 10:55:26,546 Epoch   5: total training loss 1017.47\n",
            "2020-07-17 10:55:26,546 EPOCH 6\n",
            "2020-07-17 10:55:29,139 Epoch   6 Step:     1500 Batch Loss:     2.964435 Tokens per Sec:    15386, Lr: 0.000300\n",
            "2020-07-17 10:55:42,161 Epoch   6 Step:     1600 Batch Loss:     3.327312 Tokens per Sec:    16032, Lr: 0.000300\n",
            "2020-07-17 10:55:54,911 Epoch   6 Step:     1700 Batch Loss:     3.452351 Tokens per Sec:    16667, Lr: 0.000300\n",
            "2020-07-17 10:56:04,296 Epoch   6: total training loss 971.73\n",
            "2020-07-17 10:56:04,297 EPOCH 7\n",
            "2020-07-17 10:56:07,677 Epoch   7 Step:     1800 Batch Loss:     3.329847 Tokens per Sec:    16302, Lr: 0.000300\n",
            "2020-07-17 10:56:20,216 Epoch   7 Step:     1900 Batch Loss:     3.069765 Tokens per Sec:    16043, Lr: 0.000300\n",
            "2020-07-17 10:56:32,989 Epoch   7 Step:     2000 Batch Loss:     2.819690 Tokens per Sec:    15956, Lr: 0.000300\n",
            "2020-07-17 10:56:56,446 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 10:56:56,446 Saving new checkpoint.\n",
            "2020-07-17 10:56:56,678 Example #0\n",
            "2020-07-17 10:56:56,680 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 10:56:56,680 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 10:56:56,680 \tHypothesis: Ekpako na da vwẹ ukẹcha kẹ ihwo efa , ji vwo ẹguọnọ rẹ avwanre , ji vwo ẹguọnọ rẹ avwanre .\n",
            "2020-07-17 10:56:56,680 Example #1\n",
            "2020-07-17 10:56:56,681 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 10:56:56,681 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 10:56:56,681 \tHypothesis: O vwo ọke rẹ ihwo buebun .\n",
            "2020-07-17 10:56:56,681 Example #2\n",
            "2020-07-17 10:56:56,681 \tSource:     But freedom from what ?\n",
            "2020-07-17 10:56:56,682 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 10:56:56,682 \tHypothesis: Ẹkẹvuọvo , die yen e se vwo ?\n",
            "2020-07-17 10:56:56,682 Example #3\n",
            "2020-07-17 10:56:56,682 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 10:56:56,682 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 10:56:56,682 \tHypothesis: Wọ da vwẹ ukẹcha kẹ ihwo efa .\n",
            "2020-07-17 10:56:56,683 Validation result (greedy) at epoch   7, step     2000: bleu:   3.19, loss: 73999.6406, ppl:  23.0264, duration: 23.6932s\n",
            "2020-07-17 10:57:06,088 Epoch   7: total training loss 950.76\n",
            "2020-07-17 10:57:06,088 EPOCH 8\n",
            "2020-07-17 10:57:09,592 Epoch   8 Step:     2100 Batch Loss:     3.217885 Tokens per Sec:    17006, Lr: 0.000300\n",
            "2020-07-17 10:57:22,456 Epoch   8 Step:     2200 Batch Loss:     2.889496 Tokens per Sec:    15652, Lr: 0.000300\n",
            "2020-07-17 10:57:35,169 Epoch   8 Step:     2300 Batch Loss:     2.932196 Tokens per Sec:    16253, Lr: 0.000300\n",
            "2020-07-17 10:57:44,225 Epoch   8: total training loss 911.62\n",
            "2020-07-17 10:57:44,225 EPOCH 9\n",
            "2020-07-17 10:57:48,045 Epoch   9 Step:     2400 Batch Loss:     3.420725 Tokens per Sec:    15719, Lr: 0.000300\n",
            "2020-07-17 10:58:01,001 Epoch   9 Step:     2500 Batch Loss:     3.290416 Tokens per Sec:    16246, Lr: 0.000300\n",
            "2020-07-17 10:58:13,656 Epoch   9 Step:     2600 Batch Loss:     2.563433 Tokens per Sec:    16200, Lr: 0.000300\n",
            "2020-07-17 10:58:22,069 Epoch   9: total training loss 881.46\n",
            "2020-07-17 10:58:22,070 EPOCH 10\n",
            "2020-07-17 10:58:26,571 Epoch  10 Step:     2700 Batch Loss:     2.477622 Tokens per Sec:    16105, Lr: 0.000300\n",
            "2020-07-17 10:58:39,152 Epoch  10 Step:     2800 Batch Loss:     3.053063 Tokens per Sec:    16691, Lr: 0.000300\n",
            "2020-07-17 10:58:51,957 Epoch  10 Step:     2900 Batch Loss:     2.991760 Tokens per Sec:    15969, Lr: 0.000300\n",
            "2020-07-17 10:58:59,501 Epoch  10: total training loss 851.59\n",
            "2020-07-17 10:58:59,501 EPOCH 11\n",
            "2020-07-17 10:59:04,781 Epoch  11 Step:     3000 Batch Loss:     2.903226 Tokens per Sec:    15539, Lr: 0.000300\n",
            "2020-07-17 10:59:21,816 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 10:59:21,816 Saving new checkpoint.\n",
            "2020-07-17 10:59:22,057 Example #0\n",
            "2020-07-17 10:59:22,058 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 10:59:22,058 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 10:59:22,058 \tHypothesis: Enana cha nẹrhẹ ayen riẹn oborẹ ayen sa vwọ vwẹ ukẹcha kẹ ihwo efa , ji vwo ẹruọ rẹ oborẹ ayen se vwo ruiruo rẹ aghwoghwo na .\n",
            "2020-07-17 10:59:22,058 Example #1\n",
            "2020-07-17 10:59:22,059 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 10:59:22,059 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 10:59:22,059 \tHypothesis: O vwo ọke rẹ avwanre vwọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 10:59:22,059 Example #2\n",
            "2020-07-17 10:59:22,060 \tSource:     But freedom from what ?\n",
            "2020-07-17 10:59:22,060 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 10:59:22,060 \tHypothesis: Ẹkẹvuọvo , die yen uruemu nana ?\n",
            "2020-07-17 10:59:22,060 Example #3\n",
            "2020-07-17 10:59:22,061 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 10:59:22,061 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 10:59:22,061 \tHypothesis: Wọ da vwẹ ukẹcha kẹ iniọvo wẹn .\n",
            "2020-07-17 10:59:22,061 Validation result (greedy) at epoch  11, step     3000: bleu:   5.23, loss: 67267.8594, ppl:  17.3103, duration: 17.2794s\n",
            "2020-07-17 10:59:34,735 Epoch  11 Step:     3100 Batch Loss:     3.043530 Tokens per Sec:    15869, Lr: 0.000300\n",
            "2020-07-17 10:59:47,658 Epoch  11 Step:     3200 Batch Loss:     2.895080 Tokens per Sec:    16003, Lr: 0.000300\n",
            "2020-07-17 10:59:55,147 Epoch  11: total training loss 846.40\n",
            "2020-07-17 10:59:55,147 EPOCH 12\n",
            "2020-07-17 11:00:00,504 Epoch  12 Step:     3300 Batch Loss:     2.732128 Tokens per Sec:    16051, Lr: 0.000300\n",
            "2020-07-17 11:00:13,153 Epoch  12 Step:     3400 Batch Loss:     2.332400 Tokens per Sec:    16311, Lr: 0.000300\n",
            "2020-07-17 11:00:25,907 Epoch  12 Step:     3500 Batch Loss:     2.449204 Tokens per Sec:    16471, Lr: 0.000300\n",
            "2020-07-17 11:00:32,882 Epoch  12: total training loss 817.72\n",
            "2020-07-17 11:00:32,882 EPOCH 13\n",
            "2020-07-17 11:00:38,560 Epoch  13 Step:     3600 Batch Loss:     2.711875 Tokens per Sec:    15946, Lr: 0.000300\n",
            "2020-07-17 11:00:51,163 Epoch  13 Step:     3700 Batch Loss:     2.779570 Tokens per Sec:    16450, Lr: 0.000300\n",
            "2020-07-17 11:01:04,064 Epoch  13 Step:     3800 Batch Loss:     2.662308 Tokens per Sec:    16096, Lr: 0.000300\n",
            "2020-07-17 11:01:10,404 Epoch  13: total training loss 800.78\n",
            "2020-07-17 11:01:10,404 EPOCH 14\n",
            "2020-07-17 11:01:16,593 Epoch  14 Step:     3900 Batch Loss:     3.075555 Tokens per Sec:    16086, Lr: 0.000300\n",
            "2020-07-17 11:01:29,142 Epoch  14 Step:     4000 Batch Loss:     3.021625 Tokens per Sec:    16452, Lr: 0.000300\n",
            "2020-07-17 11:01:46,600 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 11:01:46,600 Saving new checkpoint.\n",
            "2020-07-17 11:01:46,848 Example #0\n",
            "2020-07-17 11:01:46,848 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 11:01:46,848 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 11:01:46,848 \tHypothesis: Enana cha nẹrhẹ ayen vwo ẹwẹn rẹ ohwo na , rere ayen se vwo vwo ẹwẹn obrorhiẹn rẹ avwanre .\n",
            "2020-07-17 11:01:46,849 Example #1\n",
            "2020-07-17 11:01:46,849 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 11:01:46,849 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:01:46,849 \tHypothesis: Nonẹna , ọ je ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:01:46,849 Example #2\n",
            "2020-07-17 11:01:46,850 \tSource:     But freedom from what ?\n",
            "2020-07-17 11:01:46,850 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 11:01:46,850 \tHypothesis: Ẹkẹvuọvo , die yen egbomọphẹ ?\n",
            "2020-07-17 11:01:46,850 Example #3\n",
            "2020-07-17 11:01:46,850 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 11:01:46,850 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 11:01:46,851 \tHypothesis: Wọ da guọnọ ukẹcha rẹ ukoko wẹn .\n",
            "2020-07-17 11:01:46,851 Validation result (greedy) at epoch  14, step     4000: bleu:   6.52, loss: 63432.9688, ppl:  14.7133, duration: 17.7079s\n",
            "2020-07-17 11:01:59,769 Epoch  14 Step:     4100 Batch Loss:     2.430099 Tokens per Sec:    15921, Lr: 0.000300\n",
            "2020-07-17 11:02:05,941 Epoch  14: total training loss 794.39\n",
            "2020-07-17 11:02:05,941 EPOCH 15\n",
            "2020-07-17 11:02:12,264 Epoch  15 Step:     4200 Batch Loss:     1.968309 Tokens per Sec:    16180, Lr: 0.000300\n",
            "2020-07-17 11:02:24,884 Epoch  15 Step:     4300 Batch Loss:     2.261740 Tokens per Sec:    16569, Lr: 0.000300\n",
            "2020-07-17 11:02:37,454 Epoch  15 Step:     4400 Batch Loss:     2.637823 Tokens per Sec:    16234, Lr: 0.000300\n",
            "2020-07-17 11:02:43,473 Epoch  15: total training loss 775.35\n",
            "2020-07-17 11:02:43,473 EPOCH 16\n",
            "2020-07-17 11:02:50,276 Epoch  16 Step:     4500 Batch Loss:     2.282834 Tokens per Sec:    15949, Lr: 0.000300\n",
            "2020-07-17 11:03:02,953 Epoch  16 Step:     4600 Batch Loss:     2.931057 Tokens per Sec:    16508, Lr: 0.000300\n",
            "2020-07-17 11:03:15,518 Epoch  16 Step:     4700 Batch Loss:     2.601682 Tokens per Sec:    16484, Lr: 0.000300\n",
            "2020-07-17 11:03:21,027 Epoch  16: total training loss 752.91\n",
            "2020-07-17 11:03:21,027 EPOCH 17\n",
            "2020-07-17 11:03:28,465 Epoch  17 Step:     4800 Batch Loss:     2.619023 Tokens per Sec:    15637, Lr: 0.000300\n",
            "2020-07-17 11:03:41,088 Epoch  17 Step:     4900 Batch Loss:     2.600334 Tokens per Sec:    16645, Lr: 0.000300\n",
            "2020-07-17 11:03:53,765 Epoch  17 Step:     5000 Batch Loss:     3.017671 Tokens per Sec:    16427, Lr: 0.000300\n",
            "2020-07-17 11:04:11,506 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 11:04:11,507 Saving new checkpoint.\n",
            "2020-07-17 11:04:11,768 Example #0\n",
            "2020-07-17 11:04:11,768 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 11:04:11,768 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 11:04:11,769 \tHypothesis: Enana cha nẹrhẹ a riẹn oborẹ ẹwẹn rẹ avwanre se vwo ruiruo rẹ ubiudu rẹ avwanre .\n",
            "2020-07-17 11:04:11,769 Example #1\n",
            "2020-07-17 11:04:11,769 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 11:04:11,769 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:04:11,769 \tHypothesis: Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:04:11,769 Example #2\n",
            "2020-07-17 11:04:11,770 \tSource:     But freedom from what ?\n",
            "2020-07-17 11:04:11,770 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 11:04:11,770 \tHypothesis: Ẹkẹvuọvo , die yen egbomọphẹ ?\n",
            "2020-07-17 11:04:11,770 Example #3\n",
            "2020-07-17 11:04:11,770 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 11:04:11,770 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 11:04:11,770 \tHypothesis: Wo de ru ọtiọyen , wo de nene ukoko na .\n",
            "2020-07-17 11:04:11,771 Validation result (greedy) at epoch  17, step     5000: bleu:   8.58, loss: 60454.6719, ppl:  12.9683, duration: 18.0052s\n",
            "2020-07-17 11:04:16,671 Epoch  17: total training loss 736.45\n",
            "2020-07-17 11:04:16,671 EPOCH 18\n",
            "2020-07-17 11:04:24,816 Epoch  18 Step:     5100 Batch Loss:     2.872322 Tokens per Sec:    16251, Lr: 0.000300\n",
            "2020-07-17 11:04:37,250 Epoch  18 Step:     5200 Batch Loss:     2.690220 Tokens per Sec:    16471, Lr: 0.000300\n",
            "2020-07-17 11:04:49,857 Epoch  18 Step:     5300 Batch Loss:     2.783403 Tokens per Sec:    16494, Lr: 0.000300\n",
            "2020-07-17 11:04:53,932 Epoch  18: total training loss 727.28\n",
            "2020-07-17 11:04:53,932 EPOCH 19\n",
            "2020-07-17 11:05:02,427 Epoch  19 Step:     5400 Batch Loss:     2.432964 Tokens per Sec:    16299, Lr: 0.000300\n",
            "2020-07-17 11:05:15,070 Epoch  19 Step:     5500 Batch Loss:     2.461048 Tokens per Sec:    16702, Lr: 0.000300\n",
            "2020-07-17 11:05:27,328 Epoch  19 Step:     5600 Batch Loss:     2.505314 Tokens per Sec:    16661, Lr: 0.000300\n",
            "2020-07-17 11:05:31,001 Epoch  19: total training loss 717.37\n",
            "2020-07-17 11:05:31,001 EPOCH 20\n",
            "2020-07-17 11:05:39,959 Epoch  20 Step:     5700 Batch Loss:     2.540059 Tokens per Sec:    16766, Lr: 0.000300\n",
            "2020-07-17 11:05:52,615 Epoch  20 Step:     5800 Batch Loss:     2.100429 Tokens per Sec:    16170, Lr: 0.000300\n",
            "2020-07-17 11:06:05,272 Epoch  20 Step:     5900 Batch Loss:     2.717602 Tokens per Sec:    16637, Lr: 0.000300\n",
            "2020-07-17 11:06:08,242 Epoch  20: total training loss 704.46\n",
            "2020-07-17 11:06:08,242 EPOCH 21\n",
            "2020-07-17 11:06:17,813 Epoch  21 Step:     6000 Batch Loss:     2.392230 Tokens per Sec:    16406, Lr: 0.000300\n",
            "2020-07-17 11:06:34,301 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 11:06:34,301 Saving new checkpoint.\n",
            "2020-07-17 11:06:34,552 Example #0\n",
            "2020-07-17 11:06:34,554 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 11:06:34,554 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 11:06:34,554 \tHypothesis: Enana cha nẹrhẹ ayen riẹn oborẹ ayen se vwo ru obo re cha nẹrhẹ ẹwẹn rẹ avwanre ganphiyọ .\n",
            "2020-07-17 11:06:34,554 Example #1\n",
            "2020-07-17 11:06:34,554 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 11:06:34,555 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:06:34,555 \tHypothesis: Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:06:34,555 Example #2\n",
            "2020-07-17 11:06:34,555 \tSource:     But freedom from what ?\n",
            "2020-07-17 11:06:34,555 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 11:06:34,556 \tHypothesis: Ẹkẹvuọvo , die yen egbomọphẹ ?\n",
            "2020-07-17 11:06:34,556 Example #3\n",
            "2020-07-17 11:06:34,556 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 11:06:34,556 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 11:06:34,556 \tHypothesis: Wọ sa vwẹ ukoko kpokpọ vwo ruiruo rẹ ukoko na .\n",
            "2020-07-17 11:06:34,557 Validation result (greedy) at epoch  21, step     6000: bleu:   9.54, loss: 58563.8906, ppl:  11.9695, duration: 16.7432s\n",
            "2020-07-17 11:06:47,182 Epoch  21 Step:     6100 Batch Loss:     2.178725 Tokens per Sec:    16518, Lr: 0.000300\n",
            "2020-07-17 11:06:59,769 Epoch  21 Step:     6200 Batch Loss:     2.218896 Tokens per Sec:    16669, Lr: 0.000300\n",
            "2020-07-17 11:07:01,898 Epoch  21: total training loss 692.59\n",
            "2020-07-17 11:07:01,899 EPOCH 22\n",
            "2020-07-17 11:07:12,433 Epoch  22 Step:     6300 Batch Loss:     1.820897 Tokens per Sec:    16544, Lr: 0.000300\n",
            "2020-07-17 11:07:24,994 Epoch  22 Step:     6400 Batch Loss:     2.551687 Tokens per Sec:    16241, Lr: 0.000300\n",
            "2020-07-17 11:07:37,444 Epoch  22 Step:     6500 Batch Loss:     2.283972 Tokens per Sec:    16941, Lr: 0.000300\n",
            "2020-07-17 11:07:39,028 Epoch  22: total training loss 688.31\n",
            "2020-07-17 11:07:39,028 EPOCH 23\n",
            "2020-07-17 11:07:50,045 Epoch  23 Step:     6600 Batch Loss:     1.827196 Tokens per Sec:    17176, Lr: 0.000300\n",
            "2020-07-17 11:08:02,048 Epoch  23 Step:     6700 Batch Loss:     1.990705 Tokens per Sec:    16633, Lr: 0.000300\n",
            "2020-07-17 11:08:14,470 Epoch  23 Step:     6800 Batch Loss:     2.154339 Tokens per Sec:    16860, Lr: 0.000300\n",
            "2020-07-17 11:08:15,318 Epoch  23: total training loss 677.43\n",
            "2020-07-17 11:08:15,318 EPOCH 24\n",
            "2020-07-17 11:08:26,892 Epoch  24 Step:     6900 Batch Loss:     1.603740 Tokens per Sec:    16691, Lr: 0.000300\n",
            "2020-07-17 11:08:39,237 Epoch  24 Step:     7000 Batch Loss:     1.891212 Tokens per Sec:    16891, Lr: 0.000300\n",
            "2020-07-17 11:08:56,734 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 11:08:56,734 Saving new checkpoint.\n",
            "2020-07-17 11:08:57,013 Example #0\n",
            "2020-07-17 11:08:57,014 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 11:08:57,014 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 11:08:57,014 \tHypothesis: Enana cha nẹrhẹ ayen riẹn oborẹ iroro rẹ avwanre che te , ji che djobọte ubiudu rẹ avwanre .\n",
            "2020-07-17 11:08:57,015 Example #1\n",
            "2020-07-17 11:08:57,015 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 11:08:57,015 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:08:57,015 \tHypothesis: Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:08:57,016 Example #2\n",
            "2020-07-17 11:08:57,016 \tSource:     But freedom from what ?\n",
            "2020-07-17 11:08:57,016 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 11:08:57,016 \tHypothesis: Ẹkẹvuọvo , die yen egbomọphẹ ?\n",
            "2020-07-17 11:08:57,016 Example #3\n",
            "2020-07-17 11:08:57,017 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 11:08:57,017 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 11:08:57,017 \tHypothesis: Wo se vwo ẹwẹn ivivẹ kpahen ukoko kpokpọ wẹn .\n",
            "2020-07-17 11:08:57,017 Validation result (greedy) at epoch  24, step     7000: bleu:  10.83, loss: 57073.4688, ppl:  11.2367, duration: 17.7803s\n",
            "2020-07-17 11:09:09,139 Epoch  24 Step:     7100 Batch Loss:     2.404240 Tokens per Sec:    16938, Lr: 0.000300\n",
            "2020-07-17 11:09:09,488 Epoch  24: total training loss 672.60\n",
            "2020-07-17 11:09:09,488 EPOCH 25\n",
            "2020-07-17 11:09:21,166 Epoch  25 Step:     7200 Batch Loss:     2.778461 Tokens per Sec:    17089, Lr: 0.000300\n",
            "2020-07-17 11:09:33,028 Epoch  25 Step:     7300 Batch Loss:     2.337781 Tokens per Sec:    17061, Lr: 0.000300\n",
            "2020-07-17 11:09:45,329 Epoch  25 Step:     7400 Batch Loss:     2.566461 Tokens per Sec:    16923, Lr: 0.000300\n",
            "2020-07-17 11:09:45,450 Epoch  25: total training loss 670.12\n",
            "2020-07-17 11:09:45,450 EPOCH 26\n",
            "2020-07-17 11:09:57,395 Epoch  26 Step:     7500 Batch Loss:     2.255321 Tokens per Sec:    17200, Lr: 0.000300\n",
            "2020-07-17 11:10:09,920 Epoch  26 Step:     7600 Batch Loss:     2.143703 Tokens per Sec:    16688, Lr: 0.000300\n",
            "2020-07-17 11:10:21,442 Epoch  26: total training loss 650.53\n",
            "2020-07-17 11:10:21,442 EPOCH 27\n",
            "2020-07-17 11:10:22,232 Epoch  27 Step:     7700 Batch Loss:     2.102066 Tokens per Sec:    13476, Lr: 0.000300\n",
            "2020-07-17 11:10:34,316 Epoch  27 Step:     7800 Batch Loss:     1.850342 Tokens per Sec:    17032, Lr: 0.000300\n",
            "2020-07-17 11:10:46,426 Epoch  27 Step:     7900 Batch Loss:     1.903350 Tokens per Sec:    17273, Lr: 0.000300\n",
            "2020-07-17 11:10:57,322 Epoch  27: total training loss 652.07\n",
            "2020-07-17 11:10:57,322 EPOCH 28\n",
            "2020-07-17 11:10:58,553 Epoch  28 Step:     8000 Batch Loss:     1.653314 Tokens per Sec:    16854, Lr: 0.000300\n",
            "2020-07-17 11:11:12,508 Hooray! New best validation result [ppl]!\n",
            "2020-07-17 11:11:12,508 Saving new checkpoint.\n",
            "2020-07-17 11:11:12,764 Example #0\n",
            "2020-07-17 11:11:12,764 \tSource:     These orchestral arrangements are composed in such a way that they will prepare our heart and mind for the program to follow .\n",
            "2020-07-17 11:11:12,764 \tReference:  E ru uhworo nana vwẹ idjerhe ro de se muegbe rẹ ubiudu rẹ avwanre hẹrhẹ ọrhuẹrẹphiyọ rẹ ẹdẹ yena .\n",
            "2020-07-17 11:11:12,765 \tHypothesis: Enana cha nẹrhẹ ayen vwo ẹwẹn ivivẹ kpahen obo re cha nẹrhẹ e roro kpahen obo re cha nẹrhẹ e nene ayen ta ota kpahen .\n",
            "2020-07-17 11:11:12,765 Example #1\n",
            "2020-07-17 11:11:12,765 \tSource:     Today he is serving at Bethel .\n",
            "2020-07-17 11:11:12,766 \tReference:  Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:11:12,766 \tHypothesis: Nonẹna , ọ ga vwẹ Bẹtẹl .\n",
            "2020-07-17 11:11:12,766 Example #2\n",
            "2020-07-17 11:11:12,766 \tSource:     But freedom from what ?\n",
            "2020-07-17 11:11:12,766 \tReference:  Ẹkẹvuọvo , ẹdia vọ yen egbomọphẹ na che si ayen nu ?\n",
            "2020-07-17 11:11:12,767 \tHypothesis: Ẹkẹvuọvo , egbomọphẹ vọ yen egbomọphẹ ?\n",
            "2020-07-17 11:11:12,767 Example #3\n",
            "2020-07-17 11:11:12,767 \tSource:     Avoid comparing your new congregation with your previous one .\n",
            "2020-07-17 11:11:12,767 \tReference:  Wọ vwẹ ukoko kpokpọ na vwọ vwanvwen ọ rẹ wo nurhe na - a .\n",
            "2020-07-17 11:11:12,767 \tHypothesis: Wo vwo ẹwẹn ivivẹ kpahen iniọvo kpokpọ na .\n",
            "2020-07-17 11:11:12,768 Validation result (greedy) at epoch  28, step     8000: bleu:  10.80, loss: 56278.6719, ppl:  10.8645, duration: 14.2145s\n",
            "2020-07-17 11:11:24,966 Epoch  28 Step:     8100 Batch Loss:     1.349280 Tokens per Sec:    17057, Lr: 0.000300\n",
            "2020-07-17 11:11:37,239 Epoch  28 Step:     8200 Batch Loss:     1.980043 Tokens per Sec:    16875, Lr: 0.000300\n",
            "2020-07-17 11:11:47,643 Epoch  28: total training loss 642.11\n",
            "2020-07-17 11:11:47,644 EPOCH 29\n",
            "2020-07-17 11:11:49,475 Epoch  29 Step:     8300 Batch Loss:     1.196009 Tokens per Sec:    17555, Lr: 0.000300\n",
            "2020-07-17 11:12:01,640 Epoch  29 Step:     8400 Batch Loss:     2.019767 Tokens per Sec:    16974, Lr: 0.000300\n",
            "2020-07-17 11:12:13,778 Epoch  29 Step:     8500 Batch Loss:     2.040623 Tokens per Sec:    17096, Lr: 0.000300\n",
            "2020-07-17 11:12:23,566 Epoch  29: total training loss 637.24\n",
            "2020-07-17 11:12:23,567 EPOCH 30\n",
            "2020-07-17 11:12:25,939 Epoch  30 Step:     8600 Batch Loss:     1.952216 Tokens per Sec:    16733, Lr: 0.000300\n",
            "2020-07-17 11:12:38,513 Epoch  30 Step:     8700 Batch Loss:     1.487978 Tokens per Sec:    16438, Lr: 0.000300\n",
            "2020-07-17 11:12:50,644 Epoch  30 Step:     8800 Batch Loss:     2.418194 Tokens per Sec:    16970, Lr: 0.000300\n",
            "2020-07-17 11:13:00,085 Epoch  30: total training loss 636.02\n",
            "2020-07-17 11:13:00,086 Training ended after  30 epochs.\n",
            "2020-07-17 11:13:00,086 Best validation result (greedy) at step     8000:  10.86 ppl.\n",
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "2020-07-17 11:13:16,309  dev bleu:  10.88 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-07-17 11:13:16,310 Translations saved to: models/enurh_transformer/00008000.hyps.dev\n",
            "2020-07-17 11:13:46,168 test bleu:  21.12 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-07-17 11:13:46,170 Translations saved to: models/enurh_transformer/00008000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e007876d-6abe-4d35-bb32-effe69fba406"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create symbolic link '/content/drive/My Drive/masakhane/en-urh-baseline/models/enurh_transformer/best.ckpt': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "008bafc1-1067-4ff0-8442-8bf846184722"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 85283.75000\tPPL: 37.14926\tbleu: 1.16335\tLR: 0.00030000\t*\n",
            "Steps: 2000\tLoss: 73999.64062\tPPL: 23.02640\tbleu: 3.18598\tLR: 0.00030000\t*\n",
            "Steps: 3000\tLoss: 67267.85938\tPPL: 17.31027\tbleu: 5.23256\tLR: 0.00030000\t*\n",
            "Steps: 4000\tLoss: 63432.96875\tPPL: 14.71326\tbleu: 6.51980\tLR: 0.00030000\t*\n",
            "Steps: 5000\tLoss: 60454.67188\tPPL: 12.96829\tbleu: 8.57654\tLR: 0.00030000\t*\n",
            "Steps: 6000\tLoss: 58563.89062\tPPL: 11.96951\tbleu: 9.54402\tLR: 0.00030000\t*\n",
            "Steps: 7000\tLoss: 57073.46875\tPPL: 11.23673\tbleu: 10.83035\tLR: 0.00030000\t*\n",
            "Steps: 8000\tLoss: 56278.67188\tPPL: 10.86447\tbleu: 10.79909\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "5faf7df6-2f4f-402b-ac17-c2de7de9f8bc"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-07-17 11:14:05,743 Hello! This is Joey-NMT.\n",
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "2020-07-17 11:14:25,139  dev bleu:  10.88 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-07-17 11:14:53,976 test bleu:  21.12 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}